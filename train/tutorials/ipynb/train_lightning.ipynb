{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.loggers import MLFlowLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class\n",
    "class Dataset(InMemory):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        print(\"INFO: self.processed_paths = \",self.processed_paths)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels).jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels).jittable()\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels).jittable()\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = torch.sigmoid(x) #NOTE: DON'T SOFTMAX IF USING BCELOSS, USE SIGMOID INSTEAD\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generic pytorch-lightning model for binary classification\n",
    "\n",
    "class PLModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_class = None,\n",
    "                 model_class_args = [],\n",
    "                 model_class_kwargs = {},\n",
    "                 criterion = None,\n",
    "                 optimizer = None,\n",
    "                 optimizer_kwargs = None,\n",
    "                 task = \"binary\",\n",
    "                 num_classes = 1,\n",
    "                 weight = True,\n",
    "                 dataset_class = None,\n",
    "                 ds_args = [],\n",
    "                 ds_kwargs = {},\n",
    "                 lengths = [1.0],\n",
    "                 dataloader_class = None,\n",
    "                 train_batch_size = 64,\n",
    "                 val_batch_size = 64,\n",
    "                 test_batch_size = 64,\n",
    "                 num_workers = 4\n",
    "                ):\n",
    "        super(PLModel, self).__init__()\n",
    "        self.criterion = criterion if criterion is not None else F.binary_cross_entropy\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.task = task\n",
    "        if self.task!='binary': raise TypeError('PLModel: Only binary classification implemented so far')\n",
    "        self.num_classes = num_classes #NOTE: FOR BCELoss SHOULD HAVE NUM_CLASSES=1.\n",
    "        self.weight = weight #NOTE: Whether or not to use loss weighting on batch basis\n",
    "        self.dataset_class = dataset_class\n",
    "        self.ds_args = ds_args\n",
    "        self.ds_kwargs = ds_kwargs\n",
    "        self.lengths = lengths\n",
    "        self.dataloader_class = dataloader_class\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        # Init random class attributes\n",
    "        self.dataset = None\n",
    "        self.ds_train = None\n",
    "        self.ds_val = None\n",
    "        self.ds_test = None\n",
    "        \n",
    "        self.train_accuracy = Accuracy(task=self.task, num_classes=self.num_classes)\n",
    "        self.val_accuracy = Accuracy(task=self.task, num_classes=self.num_classes)\n",
    "        self.test_accuracy = Accuracy(task=self.task, num_classes=self.num_classes)\n",
    "        \n",
    "        self.model = model_class(*model_class_args,**model_class_kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = torch.squeeze(self.model(batch.x, batch.edge_index, batch.batch))\n",
    "        counts = torch.pow(torch.unique(batch.y,return_counts=True)[1] / len(batch.y), -1) if self.weight else None\n",
    "        weight = torch.tensor([counts[idx] for idx in torch.squeeze(batch.y)]).to(x.device) if self.weight else None #NOTE: THIS ONLY WORKS FOR BINARY CLASSIFICATION WITH BCELOSS\n",
    "        loss = self.criterion(x, batch.y.float(), weight=weight)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', self.train_accuracy, prog_bar=True, batch_size=self.train_batch_size)\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = torch.squeeze(self.model(batch.x, batch.edge_index, batch.batch))\n",
    "        counts = torch.pow(torch.unique(batch.y,return_counts=True)[1] / len(batch.y), -1) if self.weight else None\n",
    "        weight = torch.tensor([counts[idx] for idx in torch.squeeze(batch.y)]).to(x.device) if self.weight else None #NOTE: THIS ONLY WORKS FOR BINARY CLASSIFICATION WITH BCELOSS\n",
    "        loss = self.criterion(x, batch.y.float(), weight=weight)\n",
    "        preds = x.round() #NOTE: ONLY USE FOR BINARY CLASSIFICATION\n",
    "        self.val_accuracy.update(preds, batch.y)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('val_loss', loss, prog_bar=True, batch_size=self.val_batch_size)\n",
    "        self.log('val_acc', self.val_accuracy, prog_bar=True, batch_size=self.val_batch_size)\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def test_step(self, batch, batch_nb):\n",
    "        x = torch.squeeze(self.model(batch.x, batch.edge_index, batch.batch))\n",
    "        counts = torch.pow(torch.unique(batch.y,return_counts=True)[1] / len(batch.y), -1) if self.weight else None\n",
    "        weight = torch.tensor([counts[idx] for idx in torch.squeeze(batch.y)]).to(x.device) if self.weight else None #NOTE: THIS ONLY WORKS FOR BINARY CLASSIFICATION WITH BCELOSS\n",
    "        loss = self.criterion(x, batch.y.float(), weight=weight)\n",
    "        preds = x.round() #NOTE: ONLY USE FOR BINARY CLASSIFICATION\n",
    "        self.val_accuracy.update(preds, batch.y)\n",
    "\n",
    "        # Calling self.log will surface up scalars for you in TensorBoard\n",
    "        self.log('test_loss', loss, prog_bar=True, batch_size=self.test_batch_size)\n",
    "        self.log('test_acc', self.test_accuracy, prog_bar=True, batch_size=self.test_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer is not None and self.optimizer_kwargs is not None:\n",
    "            return self.optimizer(self.parameters(), **self.optimizer_kwargs)\n",
    "        else:\n",
    "            return torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "            \n",
    "    def prepare_data(self): #NOTE: DO NOT MAKE ANY STATE ASSIGNMENTS HERE, JUST DOWNLOAD THE DATA IF NEEDED\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None): #NOTE: THIS RUNS ACROSS ALL GPUS\n",
    "        # Assign train/val/test datasets for use in dataloaders\n",
    "        if self.dataset is None:\n",
    "            self.dataset = self.dataset_class(*self.ds_args,**self.ds_kwargs) #NOTE: NEEDED KWARGS datasetclass ds_args, ds_kwargs, lengths\n",
    "        if len(self.lengths)==2 and self.ds_train is None and self.ds_val is None:\n",
    "            self.ds_train, self.ds_val = random_split(self.dataset, self.lengths)\n",
    "        elif len(self.lengths)==3 and self.ds_train is None and self.ds_val is None:\n",
    "            self.ds_train, self.ds_val, self.ds_test = random_split(self.dataset, self.lengths)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.dataloader_class(self.ds_train, batch_size=self.train_batch_size, shuffle=True, num_workers=self.num_workers) #NOTE: NEEDED KWARGS dataloader_class train_batch_size val test...\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.dataloader_class(self.ds_val, batch_size=self.val_batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.dataloader_class(self.ds_test, batch_size=self.test_batch_size, shuffle=False, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pytorch lightning model with all the training/validation parameters\n",
    "transform = T.Compose([T.ToUndirected(),T.NormalizeFeatures()])\n",
    "plmodel = PLModel(\n",
    "         model_class = GNN,\n",
    "         model_class_args = [],\n",
    "         model_class_kwargs = {'in_channels':dataset.num_node_features,'hidden_channels':64,'out_channels':1},\n",
    "         criterion = torch.nn.functional.binary_cross_entropy,\n",
    "         optimizer = torch.optim.Adam,\n",
    "         optimizer_kwargs = {'lr':0.01},\n",
    "         task = 'binary',\n",
    "         num_classes = 1,\n",
    "         weight = True,\n",
    "         dataset_class = Dataset,\n",
    "         ds_args = ['/work/clas12/users/mfmce/pyg_datasets/'],\n",
    "         ds_kwargs = {'transform':transform, 'pre_transform':None, 'pre_filter':None},\n",
    "         lengths = [0.8,0.1,0.1],\n",
    "         dataloader_class = DataLoader,\n",
    "         train_batch_size = 16,\n",
    "         val_batch_size = 16,\n",
    "         test_batch_size = 16,\n",
    "         num_workers = 4\n",
    "        )\n",
    "\n",
    "# Sanity check\n",
    "print(type(plmodel))\n",
    "print(type(plmodel.model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "pl.seed_everything(72, workers=True)\n",
    "use_mlflow = False\n",
    "mlf_logger = MLFlowLogger(experiment_name=\"lightning_logs\", tracking_uri=\"file:./ml-runs\") if use_mlflow else None\n",
    "trainer = Trainer(\n",
    "    default_root_dir=\"./\", #NOTE: PL AUTOMATICALLY SAVES PL CHECKPOINT TO PWD UNLESS THIS OPTION IS DIFFERENT\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    max_epochs=3,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "    logger=CSVLogger(save_dir=\"logs/\") if mlflow_logger is None else mlf_logger,\n",
    "    deterministic=True, #NOTE: For reproducibility use pytorch_lightning.seed_everything and this\n",
    "    logger=mlf_logger\n",
    ")\n",
    "trainer.fit(plmodel)\n",
    "\n",
    "# Test model - pl automatically saves best and last checkpoints\n",
    "trainer.test(ckpt='best')\n",
    "\n",
    "# save for use in production environment\n",
    "script = plmodel.to_torchscript() #NOTE: Different method for pl\n",
    "torch.jit.save(script, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrth | tail"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
