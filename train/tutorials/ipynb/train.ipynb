{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset, download_url\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class\n",
    "class Dataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        print(\"INFO: self.processed_paths = \",self.processed_paths)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['some_file_1', 'some_file_2']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        # Read data into huge `Data` list.\n",
    "        data_list = None\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "# Load dataset\n",
    "root = \"/path/to/pyg_datasets\" #NOTE: DATA SHOULD BE SAVED IN <root>/processed/data.pt, create this with create_dataset.ipynb\n",
    "transform = T.Compose([T.ToUndirected(),T.NormalizeFeatures()])\n",
    "dataset = Dataset(root, transform=transform, pre_transform=None, pre_filter=None)\n",
    "\n",
    "# Sanity check\n",
    "data = dataset[0]\n",
    "print(data.x)\n",
    "print(dataset)\n",
    "if transform is not None:\n",
    "    print(transform(dataset[0]).edge_index)\n",
    "    print(transform(dataset[0]).x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GraphConv(in_channels, hidden_channels).jittable() #NOTE: NEEDED FOR DEPLOYMENT IN CMAKE\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels).jittable()\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels).jittable()\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = torch.sigmoid(x) #NOTE: DON'T SOFTMAX IF USING BCELOSS, USE SIGMOID INSTEAD\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model for binary classification\n",
    "model = GNN(in_channels=dataset.num_node_features,hidden_channels=64,out_channels=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model on device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "torch.manual_seed(12345)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "fracs = [0.8, 0.1, 0.1] #NOTE: SHOULD CHECK np.sum(fracs) == 1 and len(fracs)==3\n",
    "fracs = [torch.sum(torch.tensor(fracs[:idx])) for idx in range(1,len(fracs)+1)]\n",
    "print(fracs)\n",
    "split1, split2 = [int(len(dataset)*frac) for frac in fracs[:-1]]\n",
    "train_dataset = dataset[:split1]\n",
    "val_dataset = dataset[split1:split2]\n",
    "test_dataset = dataset[split2:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)#, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and validation routines\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(loader):\n",
    "    model.train()\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "        counts = torch.unique(data.y,return_counts=True)[1]\n",
    "        weights = counts / len(data.y)\n",
    "        weights = np.power(weights,-1)\n",
    "        weight = torch.tensor([weights[idx] for idx in torch.squeeze(data.y)]).to(device)\n",
    "        criterion = torch.nn.BCELoss(weight=weight)\n",
    "        \n",
    "        data = data.to(device)#NOTE: ADDED\n",
    "        out = torch.squeeze(model(data.x, data.edge_index, data.batch))  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y.float())  # Compute the loss.\n",
    "        \n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "@torch.no_grad()\n",
    "def val(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    loss_tot = 0.0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        counts = torch.unique(data.y,return_counts=True)[1]\n",
    "        weights = counts / len(data.y)\n",
    "        weights = np.power(weights,-1)\n",
    "        weight = torch.tensor([weights[idx] for idx in torch.squeeze(data.y)]).to(device)\n",
    "        criterion = torch.nn.BCELoss(weight=weight)\n",
    "        \n",
    "        data = data.to(device)\n",
    "        out = torch.squeeze(model(data.x, data.edge_index, data.batch))\n",
    "        loss = criterion(out, data.y.float())\n",
    "#         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        pred = out.round() #NOTE: JUST FOR USING BCELOSS -> ARGMAX COLLAPSES TO A ONE ELEMENT TENSOR\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        loss_tot += loss.item()\n",
    "    return correct / len(loader.dataset), loss_tot / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "# Train and test the model\n",
    "nepochs = 5\n",
    "for epoch in range(1, nepochs+1):\n",
    "    train(train_loader)\n",
    "    train_acc, train_loss = val(train_loader)\n",
    "    val_acc, val_loss = val(val_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f} Loss: {train_loss:.4f}, Val Acc: {val_acc:.4f} Loss: {val_loss:.4f}')\n",
    "    \n",
    "test_acc, test_loss = val(test_loader)\n",
    "print(f'Test Acc: {train_acc:.4f} Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
